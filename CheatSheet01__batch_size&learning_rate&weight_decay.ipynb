{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# このコードの目的\n",
    "\n",
    "１）pytorchの学習プロセス（＝パラメータの調整）について確認する。\n",
    "２）上記の確認結果に基づいて、基本的な調整パラメータであるbatch_size, learning_rate(lr), weight_decay(正則化係数)について挙動を確認する。\n",
    "\n",
    "# 結論\n",
    "batch_sizeは1が最もよく、そのbatch集合ごとの学習でパラメータが特徴を捉えやすいようにマネージする必要がある。\n",
    "batch_sizeを全学習データにしても、簡単な問題でさえ特徴を捉えきれずパラメータは収束しない。\n",
    "\n",
    "また、learning_rateとweight_decayに従って、パラメータのepochごとの調整挙動が違う。\n",
    "以下のスクリプトの出力を確認した上で、適切な設定を選ぶべき。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([5., 2., 3., 6., 6., 1., 4., 0., 0., 3.])\n"
     ]
    }
   ],
   "source": [
    "# toy data を生成する\n",
    "\n",
    "x = torch.tensor([\n",
    "    [1,0,1,1],\n",
    "    [0,0,1,0],\n",
    "    [1,0,0,1],\n",
    "    [0,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [0,1,0,0],\n",
    "    [0,1,0,1],\n",
    "    [1,0,0,0],\n",
    "    [0,0,0,0],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "## y は明確な正解となるパラメータがあるように定義する。\n",
    "y = 0 * x[:,0] + 1 * x[:,1] + 2 * x[:,2] + 3 * x[:,3]\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプルなネットワークを定義する\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4,  1, bias=None),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率(lr)と正則化項(weight_decay)ごとにoptimizerを定義する。\n",
    "criterion = torch.nn.L1Loss(reduction='sum') # loss関数はL1ロスのbatch合計とする。\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0)\n",
    "optimizer__weight_decay = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓modelのパラメータを出力↓\n",
      "pram=Parameter containing:\n",
      "tensor([[0.2211, 0.1589, 0.3895, 0.8993]], requires_grad=True)\n",
      "grad=tensor([[0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# パラメータの確認関数を定義しておく。\n",
    "\n",
    "def print_parameters(model):\n",
    "    # 'パラメータと微分値を確認する\n",
    "    print('↓modelのパラメータを出力↓')\n",
    "    for pram in model.parameters():\n",
    "        print('pram={}'.format(pram))\n",
    "        print('grad={}'.format(pram.grad))\n",
    "\n",
    "print_parameters(model) #gradはまだ計算されていない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# _xと_yとlossの確認\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor(2.)\n",
      "tensor(0.1429, grad_fn=<L1LossBackward>)\n",
      "\n",
      "# loss.backword()でparametorsごとにgradを計算されます\n",
      "↓modelのパラメータを出力↓\n",
      "pram=Parameter containing:\n",
      "tensor([[0.2125, 0.1542, 0.7739, 0.8706]], requires_grad=True)\n",
      "grad=tensor([[-0.9785,  0.0156, -2.9420, -0.9121]])\n",
      "\n",
      "# optimizer.step() でoptimizerがパラメータを更新します\n",
      "### 正則化なしのoptimizer ###\n",
      "↓modelのパラメータを出力↓\n",
      "pram=Parameter containing:\n",
      "tensor([[0.3104, 0.1526, 1.0681, 0.9618]], requires_grad=True)\n",
      "grad=tensor([[-0.9785,  0.0156, -2.9420, -0.9121]])\n",
      "     --> SGDなので、pram = pram - lr * pram.grad となる\n",
      "### 正則化ありのoptimizer ###\n",
      "↓modelのパラメータを出力↓\n",
      "pram=Parameter containing:\n",
      "tensor([[0.4051, 0.1495, 1.3517, 1.0434]], requires_grad=True)\n",
      "grad=tensor([[-0.9475,  0.0308, -2.8352, -0.8159]])\n",
      "     --> SGDなので、pram = pram - (lr * pram.grad + weight_decay * param)  となる\n",
      "\n",
      "# optimizer.zero_grad() でgradを0にリセットする\n",
      "↓modelのパラメータを出力↓\n",
      "pram=Parameter containing:\n",
      "tensor([[0.4051, 0.1495, 1.3517, 1.0434]], requires_grad=True)\n",
      "grad=tensor([[0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# optimizerの挙動を確認する。\n",
    "_x = x[0]\n",
    "_y = y[0]\n",
    "loss = criterion(model(_x), _y)\n",
    "\n",
    "print('# _xと_yとlossの確認')\n",
    "print(_x)\n",
    "print(_y)\n",
    "print(loss)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('# loss.backword()でparametorsごとにgradを計算されます')\n",
    "loss.backward()\n",
    "print_parameters(model) # この時gradがL1lossの微分定義に従い、±1*_x になることを確認\n",
    "\n",
    "print('')\n",
    "\n",
    "print('# optimizer.step() でoptimizerがパラメータを更新します')\n",
    "print('### 正則化なしのoptimizer ###')\n",
    "optimizer.step()\n",
    "print_parameters(model) \n",
    "print('     --> SGDなので、pram = pram - lr * pram.grad となる')\n",
    "print('### 正則化ありのoptimizer ###')\n",
    "optimizer__weight_decay.step()\n",
    "print_parameters(model) \n",
    "print('     --> SGDなので、pram = pram - (lr * pram.grad + weight_decay * param)  となる')\n",
    "\n",
    "print('')\n",
    "\n",
    "print('# optimizer.zero_grad() でgradを0にリセットする')\n",
    "optimizer.zero_grad()\n",
    "print_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# xとyとlossの確認\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 1.],\n",
      "        [0., 1., 0., 1.]])\n",
      "tensor([2., 2., 0., 1.])\n",
      "tensor(13.2137, grad_fn=<L1LossBackward>)\n",
      "\n",
      "# loss.backword()でparametorsごとにgradを計算されます\n",
      "↓modelのパラメータを出力↓\n",
      "pram=Parameter containing:\n",
      "tensor([[0.0051, 0.1495, 0.9517, 0.6434]], requires_grad=True)\n",
      "grad=tensor([[ 2., -2.,  2.,  0.]])\n",
      "     --> gradはbatch全てのエラー合計に応じて計算される\n",
      "\n",
      "# optimizer.step() でoptimizerがパラメータを更新します\n",
      "### 正則化なしのoptimizer ###\n",
      "↓modelのパラメータを出力↓\n",
      "pram=Parameter containing:\n",
      "tensor([[-0.1949,  0.3495,  0.7517,  0.6434]], requires_grad=True)\n",
      "grad=tensor([[ 2., -2.,  2.,  0.]])\n",
      "     --> パラメータの更新もSGDに基づいて一括変換される\n",
      "       --> よって、batch.sizeが大きすぎると学習サンプルごとの特徴を捉えきれない可能性が高い\n"
     ]
    }
   ],
   "source": [
    "# しかし、batch_sizeをフルサイズにすると、以下のようになる。\n",
    "\n",
    "    \n",
    "loss = criterion(model(x), y)\n",
    "\n",
    "print('# xとyとlossの確認')\n",
    "print(x)\n",
    "print(y)\n",
    "print(loss)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('# loss.backword()でparametorsごとにgradを計算されます')\n",
    "loss.backward()\n",
    "print_parameters(model)\n",
    "print('     --> gradはbatch全てのエラー合計に応じて計算される')\n",
    "\n",
    "print('')\n",
    "\n",
    "print('# optimizer.step() でoptimizerがパラメータを更新します')\n",
    "print('### 正則化なしのoptimizer ###')\n",
    "optimizer.step()\n",
    "print_parameters(model) \n",
    "print('     --> パラメータの更新もSGDに基づいて一括変換される')\n",
    "print('       --> よって、batch.sizeが大きすぎると学習サンプルごとの特徴を捉えきれない可能性がある')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_sizeを最大で学習する\n",
      "↓modelのパラメータを出力↓\n",
      "pram=Parameter containing:\n",
      "tensor([[ 0.1051,  0.1495, -0.1483, -1.2567]], requires_grad=True)\n",
      "grad=tensor([[32., 32., 32., 52.]])\n",
      "  ----> 正解のパラメータである[0,1,2,3]に収束できない\n"
     ]
    }
   ],
   "source": [
    "print('batch_sizeを最大で学習する')\n",
    "for t in range(1000):\n",
    "    loss = criterion(model(x), y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print_parameters(model)\n",
    "print('  ----> 正解のパラメータである[0,1,2,3]に収束できない')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=1で学習する\n",
      "↓modelのパラメータを出力↓\n",
      "pram=Parameter containing:\n",
      "tensor([[-0.0949,  0.8495,  2.0517,  2.9433]], requires_grad=True)\n",
      "grad=tensor([[1., 1., 1., 1.]])\n",
      "  ----> 正解のパラメータである[0,1,2,3]に収束できない\n"
     ]
    }
   ],
   "source": [
    "print('batch_size=1で学習する')\n",
    "import random\n",
    "for t in range(1000):\n",
    "    batch_index = random.choice(range(x.shape[0]))\n",
    "    loss = criterion(model(x[batch_index]), y[batch_index])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print_parameters(model)\n",
    "print('  ----> 正解のパラメータである[0,1,2,3]に収束できた')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
